{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mWQm8TwBooG5",
    "outputId": "e3702e40-0e5d-4a8d-cfa0-025b7ff1b436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting treebank\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/7c/5da69ece0f2095f6501e65202755915547f8b12e5cab5fae4b2ef30be761/treebank-0.0.0-py3-none-any.whl (2.0MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0MB 5.1MB/s \n",
      "\u001b[?25hInstalling collected packages: treebank\n",
      "Successfully installed treebank-0.0.0\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "!pip install --upgrade treebank\n",
    "!pip install nltk\n",
    "import treebank\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.utils import to_categorical\n",
    "#import libraries for model building\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Embedding, SimpleRNN\n",
    "# import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdD33KvfvZD4"
   },
   "source": [
    "**Loading Penn Treebank Dataset, and taking a part of it as training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k3XCM09QooHV",
    "outputId": "093dda80-9ce0-418c-9640-eb66d5c77c76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(treebank.penn['train']) #characters\n",
    "treebank.penn['train'][0:10]\n",
    "type(treebank.penn['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsDrptzHooHY"
   },
   "outputs": [],
   "source": [
    "data = treebank.penn['train']\n",
    "data_train = data[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oS0ZSQGviVP"
   },
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gxOoogTbooHa",
    "outputId": "ac3a367a-03ee-4aa1-eda5-75d4ae78abde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', 'pierre', '<', 'unk', '>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', 'mr.', '<', 'unk', '>', 'is', 'chairman', 'of', '<', 'unk']\n",
      "['ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', 'pierre', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', 'mr.', 'is', 'chairman', 'of', 'n.v.', 'the', 'dutch', 'publishing', 'group', 'rudolph', 'N', 'years']\n"
     ]
    }
   ],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "stop_words = set()\n",
    "stop_words.add('unk')\n",
    "stop_words.add('>')\n",
    "stop_words.add('<')\n",
    "word_tokens = word_tokenize(data_train)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(word_tokens[10:50])\n",
    "print(filtered_sentence[10:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzgXJ84AooHc"
   },
   "outputs": [],
   "source": [
    "data_f = \" \"\n",
    "data_final = data_f.join(filtered_sentence)\n",
    "# len(filtered_sentence)\n",
    "# print(data_final)\n",
    "# print(type(data_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_2v110mvpV3"
   },
   "source": [
    "**Tokenzing : Converting sentences into tokens, and giving numbers to them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJm2CsmdooHg"
   },
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=None,filters='#$%&\"()*+-<=>@[\\\\]^_`{|}~\\t\\n',lower = True, split = ' ')\n",
    "tokenizer.fit_on_texts([data_final])\n",
    "\n",
    "encoded=tokenizer.texts_to_sequences([data_final])[0]\n",
    "vocab_size=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6NmqIV4v107"
   },
   "source": [
    "**Creating input and output featues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwOkom-sooHk"
   },
   "outputs": [],
   "source": [
    "#create seperate lists of 2 sequences since we will use 1 of them as input and one as output\n",
    "sequences=[]\n",
    "for i in range(len(encoded)-2):\n",
    "    sequences.append(encoded[i:i+2])\n",
    "# sequences[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HeiyCfpAooHl"
   },
   "outputs": [],
   "source": [
    "#create dependent and independent variable\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "# x = sequences[:,0]\n",
    "# y = sequences[:,1]\n",
    "for i in range(len(sequences)):\n",
    "    x.append(sequences[i][0])\n",
    "    y.append(sequences[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyCy8dP3ooHl"
   },
   "outputs": [],
   "source": [
    "#convert list to an array\n",
    "x=np.array(x)\n",
    "y=np.array(y)\n",
    "# One hot encoding\n",
    "y=to_categorical(y,vocab_size)\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7D8sGKcooHp",
    "outputId": "27ce55c7-1d91-435f-b5cf-84189e5f7449"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17268, 3107)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_test = x[:200000]\n",
    "# y_test = y[:200000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QsbL1XgwMzM"
   },
   "source": [
    "**Define Single Layer Simple RNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyzJWqLFooHp"
   },
   "outputs": [],
   "source": [
    "#start building model\n",
    "model=Sequential()\n",
    "#add Embedding layer\n",
    "model.add(Embedding(input_dim=vocab_size,output_dim=10,input_length=1))\n",
    "#add LSTM layer\n",
    "model.add(SimpleRNN(60))\n",
    "#output layer\n",
    "model.add(Dense(vocab_size,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dr807KyKooHr"
   },
   "outputs": [],
   "source": [
    "#compile \n",
    "model.compile('adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRKh0lzQooHr",
    "outputId": "96983353-1d47-42af-95a9-5adf9ecd0f36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12988/12988 [==============================] - 40s 3ms/step - loss: 7.0136 - accuracy: 0.0602 - val_loss: 7.6543 - val_accuracy: 0.0625\n",
      "Epoch 2/20\n",
      "12988/12988 [==============================] - 40s 3ms/step - loss: 6.1915 - accuracy: 0.0933 - val_loss: 8.6021 - val_accuracy: 0.0779\n",
      "Epoch 3/20\n",
      "12988/12988 [==============================] - 40s 3ms/step - loss: 5.9180 - accuracy: 0.1244 - val_loss: 9.2845 - val_accuracy: 0.0767\n",
      "Epoch 4/20\n",
      "12988/12988 [==============================] - 40s 3ms/step - loss: 5.8370 - accuracy: 0.1490 - val_loss: 9.3259 - val_accuracy: 0.0807\n",
      "Epoch 5/20\n",
      "12988/12988 [==============================] - 41s 3ms/step - loss: 5.5964 - accuracy: 0.1755 - val_loss: 9.5196 - val_accuracy: 0.0828\n",
      "Epoch 6/20\n",
      "12988/12988 [==============================] - 40s 3ms/step - loss: 5.5197 - accuracy: 0.1899 - val_loss: 9.6156 - val_accuracy: 0.0739\n",
      "Epoch 7/20\n",
      "12988/12988 [==============================] - 41s 3ms/step - loss: 5.4291 - accuracy: 0.1980 - val_loss: 9.7454 - val_accuracy: 0.0819\n",
      "Epoch 8/20\n",
      "12988/12988 [==============================] - 41s 3ms/step - loss: 5.3164 - accuracy: 0.2054 - val_loss: 9.8413 - val_accuracy: 0.0816\n",
      "Epoch 9/20\n",
      "12988/12988 [==============================] - 43s 3ms/step - loss: 5.3618 - accuracy: 0.1986 - val_loss: 9.9307 - val_accuracy: 0.0813\n",
      "Epoch 10/20\n",
      "12988/12988 [==============================] - 42s 3ms/step - loss: 5.1610 - accuracy: 0.2205 - val_loss: 9.9664 - val_accuracy: 0.0674\n",
      "Epoch 11/20\n",
      "12988/12988 [==============================] - 43s 3ms/step - loss: 5.1374 - accuracy: 0.2165 - val_loss: 10.1147 - val_accuracy: 0.0742\n",
      "Epoch 12/20\n",
      "12988/12988 [==============================] - 41s 3ms/step - loss: 5.0909 - accuracy: 0.2172 - val_loss: 10.1346 - val_accuracy: 0.0807\n",
      "Epoch 13/20\n",
      "12988/12988 [==============================] - 41s 3ms/step - loss: 4.9363 - accuracy: 0.2325 - val_loss: 10.1350 - val_accuracy: 0.0757\n",
      "Epoch 14/20\n",
      "12988/12988 [==============================] - 42s 3ms/step - loss: 4.9184 - accuracy: 0.2332 - val_loss: 10.0230 - val_accuracy: 0.0791\n",
      "Epoch 15/20\n",
      "12988/12988 [==============================] - 41s 3ms/step - loss: 4.8437 - accuracy: 0.2388 - val_loss: 10.0047 - val_accuracy: 0.0779\n",
      "Epoch 16/20\n",
      "12988/12988 [==============================] - 41s 3ms/step - loss: 4.8331 - accuracy: 0.2348 - val_loss: 9.9956 - val_accuracy: 0.0739\n",
      "Epoch 17/20\n",
      "12988/12988 [==============================] - 42s 3ms/step - loss: 4.7623 - accuracy: 0.2396 - val_loss: 10.0336 - val_accuracy: 0.0751\n",
      "Epoch 18/20\n",
      "12988/12988 [==============================] - 40s 3ms/step - loss: 4.6489 - accuracy: 0.2495 - val_loss: 9.9861 - val_accuracy: 0.0714\n",
      "Epoch 19/20\n",
      "12988/12988 [==============================] - 40s 3ms/step - loss: 4.6376 - accuracy: 0.2485 - val_loss: 9.9922 - val_accuracy: 0.0720\n",
      "Epoch 20/20\n",
      "12988/12988 [==============================] - 41s 3ms/step - loss: 4.6194 - accuracy: 0.2548 - val_loss: 10.0092 - val_accuracy: 0.0776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc068371a10>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training model\n",
    "model.fit(x,y,batch_size=1,epochs=20,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1M_efw1BwUhe"
   },
   "source": [
    "**Predict function, which predicts `n_words` from the given `text`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1XZX6f7ooHs"
   },
   "outputs": [],
   "source": [
    "#testing function\n",
    "def predict_next_word(model,tokenizer,text,n_words):\n",
    "    result=''\n",
    "    out_word=[]\n",
    "    \n",
    "    for i in range(n_words):\n",
    "#         input_data=text.split()\n",
    "        encoded_data=tokenizer.texts_to_sequences([text])[0]\n",
    "        encoded_data=np.array(encoded_data)\n",
    "        encoded_data=encoded_data.reshape(1,1)\n",
    "        \n",
    "\n",
    "        output=np.argmax(model.predict(encoded_data))\n",
    "        \n",
    "        \n",
    "        for index,word in tokenizer.word_index.items():\n",
    "            if word==output:\n",
    "                out_word.append(index)\n",
    "                text=index\n",
    "    result=' '.join(out_word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6KwUyEKwl4_"
   },
   "source": [
    "**Testing model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "UXCEwUJGooHs",
    "outputId": "85c9def4-0e27-4dca-9395-ad6ec46107c9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'funds were at the u.s. and other n n n'"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing \n",
    "predict_next_word(model,tokenizer,\"harvard\",10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "word_level_rnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
